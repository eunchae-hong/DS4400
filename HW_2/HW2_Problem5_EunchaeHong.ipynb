{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecb9489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS4400 HW 2\n",
    "# Problem 5: Gradient Descent\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "house_data = pd.read_csv(\"kc_house_data.csv\")\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca520fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent based on algorithm from class\n",
    "def gradient_descent(X, y, alpha, iterations):\n",
    "    ''' function that implements a gradient descent for training linear regression'''\n",
    "    N, d = X.shape\n",
    "    theta = np.zeros(d)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        y_pred = X @ theta\n",
    "        gradient = (2 / N) * X.T @ (y_pred - y)\n",
    "        theta = theta - alpha * gradient\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732ed4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Learning Rate (Alpha)  Iterations  \\\n",
      "0                   0.01          10   \n",
      "1                   0.01          50   \n",
      "2                   0.01         100   \n",
      "3                   0.10          10   \n",
      "4                   0.10          50   \n",
      "5                   0.10         100   \n",
      "6                   0.50          10   \n",
      "7                   0.50          50   \n",
      "8                   0.50         100   \n",
      "\n",
      "                                               Theta      Train MSE  \\\n",
      "0  [95.19802483770326, -0.7597598995880689, 11.92...   2.064963e+11   \n",
      "1  [330.8955303896301, 2.150484670225977, 6.00795...   1.617032e+11   \n",
      "2  [451.3976498338788, 5.4157369361358505, -3.679...   9.027097e+10   \n",
      "3  [464.5357166904187, 5.767519594948174, -4.6630...   8.642122e+10   \n",
      "4  [520.4074063912901, 8.356439804763589, -12.719...   6.180966e+10   \n",
      "5  [520.4148338939906, 8.445651779086882, -12.803...   7.117774e+10   \n",
      "6  [520.4148342410479, 2726836.357778585, -406506...   3.492605e+24   \n",
      "7  [2.2900625685483e+18, 2.5630901438638715e+31, ...   3.085820e+74   \n",
      "8  [3.8471581693135607e+49, 4.218434978975545e+62...  8.358831e+136   \n",
      "\n",
      "   Train R Squared       Test MSE  Test R Squared  \n",
      "0    -1.793479e+06   3.313686e+11   -1.987492e+06  \n",
      "1    -1.404439e+06   2.155668e+11   -1.292932e+06  \n",
      "2    -7.840288e+05   1.139466e+11   -6.834312e+05  \n",
      "3    -7.505926e+05   1.076701e+11   -6.457861e+05  \n",
      "4    -5.368339e+05   1.526165e+11   -9.153669e+05  \n",
      "5    -6.181985e+05   2.268904e+11   -1.360849e+06  \n",
      "6    -3.033429e+19   6.315299e+24   -3.787809e+19  \n",
      "7    -2.680125e+69   5.579756e+74   -3.346643e+69  \n",
      "8   -7.259889e+131  1.511438e+137  -9.065346e+131  \n"
     ]
    }
   ],
   "source": [
    "# vary the value of the learning rate (at least 3 different values {0.01,0.1,0.5} and report the value of the model \n",
    "# parameter theta after different number of iterations (10, 50, and 100)\n",
    "# set constants\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "num_iterations = [10, 50, 100]\n",
    "\n",
    "# get the training and testing data from the csv files, and drop features mentioned in the homework document\n",
    "house_features = house_data.drop(columns = [\"id\", \"date\", \"zipcode\", \"price\"])\n",
    "house_price = house_data[\"price\"].div(1000)\n",
    "\n",
    "X_train = train_data.drop(columns = [\"zipcode\", \"price\"])\n",
    "y_train = train_data[\"price\"].div(1000)\n",
    "\n",
    "X_test = test_data.drop(columns = [\"id\", \"date\", \"zipcode\", \"price\"])\n",
    "y_test = test_data[\"price\"].div(1000)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_test = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# scale the data to get rid of the error messages\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[:, 1:])\n",
    "X_test_scaled = scaler.transform(X_test[:, 1:])\n",
    "\n",
    "X_train_scaled = np.c_[np.ones(X_train_scaled.shape[0]), X_train_scaled]\n",
    "X_test_scaled = np.c_[np.ones(X_test_scaled.shape[0]), X_test_scaled]\n",
    "\n",
    "# initialize empty list\n",
    "gd_results = []\n",
    "\n",
    "# iterate through the different learning rates with the different number of iterations\n",
    "for alpha in learning_rates:\n",
    "    for iterations in num_iterations:\n",
    "        theta = gradient_descent(X_train_scaled, y_train, alpha, iterations)\n",
    "\n",
    "        y_train_pred = X_train @ theta\n",
    "        y_test_pred = X_test @ theta\n",
    "\n",
    "        # get the MSE and r squared metrics for training and testing\n",
    "        train_mse = round(mean_squared_error(y_train, y_train_pred), 2)\n",
    "        train_r2 = round(r2_score(y_train, y_train_pred), 2)\n",
    "\n",
    "        test_mse = round(mean_squared_error(y_test, y_test_pred), 2)\n",
    "        test_r2 = round(r2_score(y_test, y_test_pred), 2)\n",
    "\n",
    "        gd_results.append([alpha, iterations, theta, train_mse, train_r2, test_mse, test_r2])\n",
    "\n",
    "metric_table = pd.DataFrame(\n",
    "    gd_results,\n",
    "    columns = [\"Learning Rate (Alpha)\", \"Iterations\", \"Theta\", \"Train MSE\", \"Train R Squared\", \"Test MSE\", \"Test R Squared\"]\n",
    ")\n",
    "print(metric_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31199a5b",
   "metadata": {},
   "source": [
    "Write some observations about the behavior of the algorithm: How do the metrics change with different learning rates; How many iterations are needed; Does the algorithm converge to the optimal solution, etc.\n",
    "\n",
    "With a learning rate of 0.01, as we increase the number of iterations, the training and testing MSE slightly decreases, while the $R^2$ increases. Next with a learning rate of 0.1, as we increase the number of iterations the the traning and testing MSE also continue to decrease, with $R^2$ showing similar patterns. However when we increase the learning rate to 0.5, all the metrics become large, not following the previous pattern. Therefore with this algorithm it is not able to converge to the optimal solution, and in order to do this we would need to investigate with different learning rates.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
